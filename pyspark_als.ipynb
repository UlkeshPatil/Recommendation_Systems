{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WkcFc6K0OGIe"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKJzuun3PvGX",
        "outputId": "1f31fbc6-5ef4-4bcd-c22d-f64aeb478307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files:\n",
            "artist_data.txt\n",
            "README.txt\n",
            "user_artist_data.txt\n",
            "artist_alias.txt\n",
            "Files downloaded and extracted successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Set the URL for the file to be downloaded\n",
        "url = \"https://storage.googleapis.com/aas-data-sets/profiledata_06-May-2005.tar.gz\"\n",
        "\n",
        "# Set the destination folder for extraction\n",
        "destination_folder = \"data\"\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Set the filename based on the URL\n",
        "filename = os.path.join(destination_folder, \"profiledata_06-May-2005.tar.gz\")\n",
        "\n",
        "# Download the file\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the files\n",
        "with tarfile.open(filename, \"r:gz\") as tar:\n",
        "    members = tar.getmembers()\n",
        "    tar.extractall(destination_folder)\n",
        "# Print the extracted filenames\n",
        "extracted_files = [os.path.basename(member.name) for member in members if member.isfile()]\n",
        "print(\"Extracted files:\")\n",
        "for file in extracted_files:\n",
        "    print(file)\n",
        "\n",
        "print(\"Files downloaded and extracted successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hq3mD-8LQuFJ"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.config(\"spark.driver.memory\", \"12g\").appName('music-recommender').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KoJy8q7RtMj",
        "outputId": "51b45697-721c-4239-ef5d-c7ae51b6a8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|              value|\n",
            "+-------------------+\n",
            "|       1000002 1 55|\n",
            "| 1000002 1000006 33|\n",
            "|  1000002 1000007 8|\n",
            "|1000002 1000009 144|\n",
            "|1000002 1000010 314|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_user_artist_path = \"/content/data/profiledata_06-May-2005/user_artist_data.txt\"\n",
        "raw_user_artist_data = spark.read.text(raw_user_artist_path)\n",
        "\n",
        "raw_user_artist_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtw6X6L7R9hM",
        "outputId": "ae645ee7-9d35-41f3-e6ab-9807b5544361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|1134999\t06Crazy Life|\n",
            "|6821360\tPang Nakarin|\n",
            "|10113088\tTerfel, ...|\n",
            "|10151459\tThe Flam...|\n",
            "|6826647\tBodenstan...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_artist_data = spark.read.text(\"/content/data/profiledata_06-May-2005/artist_data.txt\")\n",
        "\n",
        "raw_artist_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4QtPeitSCvn",
        "outputId": "67f528f1-150c-4c22-dd11-4fa177081bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|           value|\n",
            "+----------------+\n",
            "| 1092764\t1000311|\n",
            "| 1095122\t1000557|\n",
            "| 6708070\t1007267|\n",
            "|10088054\t1042317|\n",
            "| 1195917\t1042317|\n",
            "+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_artist_alias = spark.read.text(\"/content/data/profiledata_06-May-2005/artist_alias.txt\")\n",
        "\n",
        "raw_artist_alias.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSteN00jSXUB"
      },
      "source": [
        "**Preparing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCA63rX7SL45",
        "outputId": "941c7421-bd68-4e94-9fad-68be6614123e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|              value|\n",
            "+-------------------+\n",
            "|       1000002 1 55|\n",
            "| 1000002 1000006 33|\n",
            "|  1000002 1000007 8|\n",
            "|1000002 1000009 144|\n",
            "|1000002 1000010 314|\n",
            "|  1000002 1000013 8|\n",
            "| 1000002 1000014 42|\n",
            "| 1000002 1000017 69|\n",
            "|1000002 1000024 329|\n",
            "|  1000002 1000025 1|\n",
            "+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_user_artist_data.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kltUZMQVXvM"
      },
      "source": [
        "Each line of the file contains a user ID, an artist ID, and a play count, separated by spaces. To compute statistics on the user ID, we split the line by space characters and parse the values as integers. The result is conceptually three “columns”: a user ID, artist ID, and count as ints. It makes sense to transform this to a dataframe with columns named “user”, “artist”, and “count” because it then becomes simple to compute simple statistics like the maximum and minimum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWX4nF76SO5Z",
        "outputId": "ed95f2d5-6138-42ff-918c-94ef46dc3a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+-----------+\n",
            "|min(user)|max(user)|min(artist)|max(artist)|\n",
            "+---------+---------+-----------+-----------+\n",
            "|       90|  2443548|          1|   10794401|\n",
            "+---------+---------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import split, min, max\n",
        "from pyspark.sql.types import IntegerType, StringType\n",
        "\n",
        "user_artist_df = raw_user_artist_data.withColumn('user',\n",
        "                                    split(raw_user_artist_data['value'], ' ').\\\n",
        "                                    getItem(0).\\\n",
        "                                    cast(IntegerType()))\n",
        "user_artist_df = user_artist_df.withColumn('artist',\n",
        "                                    split(raw_user_artist_data['value'], ' ').\\\n",
        "                                    getItem(1).\\\n",
        "                                    cast(IntegerType()))\n",
        "user_artist_df = user_artist_df.withColumn('count',\n",
        "                                    split(raw_user_artist_data['value'], ' ').\\\n",
        "                                    getItem(2).\\\n",
        "                                    cast(IntegerType())).drop('value')\n",
        "\n",
        "user_artist_df.select([min(\"user\"), max(\"user\"), min(\"artist\"),\\\n",
        "                                    max(\"artist\")]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1barOKLVhDj"
      },
      "source": [
        "The maximum user and artist IDs are 2443548 and 10794401, respectively (and their minimums are 90 and 1; no negative values). These are comfortably smaller than 2147483647. No additional transformation will be necessary to use these IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERrHATMSSSRa",
        "outputId": "e96eea4d-c36f-456f-9a56-92859170eb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|      id|                name|\n",
            "+--------+--------------------+\n",
            "| 1134999|        06Crazy Life|\n",
            "| 6821360|        Pang Nakarin|\n",
            "|10113088|Terfel, Bartoli- ...|\n",
            "|10151459| The Flaming Sidebur|\n",
            "| 6826647|   Bodenstandig 3000|\n",
            "+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "artist_by_id = raw_artist_data.withColumn('id', split(col('value'), '\\s+', 2).\\\n",
        "                                                getItem(0).\\\n",
        "                                                cast(IntegerType()))\n",
        "artist_by_id = artist_by_id.withColumn('name', split(col('value'), '\\s+', 2).\\\n",
        "                                               getItem(1).\\\n",
        "                                               cast(StringType())).drop('value')\n",
        "\n",
        "artist_by_id.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27boN50-S3oB",
        "outputId": "505653d8-2957-41cc-fd2d-9c03180dc10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+\n",
            "|  artist|  alias|\n",
            "+--------+-------+\n",
            "| 1092764|1000311|\n",
            "| 1095122|1000557|\n",
            "| 6708070|1007267|\n",
            "|10088054|1042317|\n",
            "| 1195917|1042317|\n",
            "+--------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "artist_alias = raw_artist_alias.withColumn('artist',\n",
        "                                          split(col('value'), '\\s+').\\\n",
        "                                                getItem(0).\\\n",
        "                                                cast(IntegerType())).\\\n",
        "                                withColumn('alias',\n",
        "                                            split(col('value'), '\\s+').\\\n",
        "                                            getItem(1).\\\n",
        "                                            cast(StringType())).\\\n",
        "                                drop('value')\n",
        "\n",
        "artist_alias.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6nrF2DdS700",
        "outputId": "b7d10bbe-c5b6-4e72-c663-e7fe4e741cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+\n",
            "|     id|          name|\n",
            "+-------+--------------+\n",
            "|1000311| Steve Winwood|\n",
            "|1092764|Winwood, Steve|\n",
            "+-------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "artist_by_id.filter(artist_by_id.id.isin(1092764, 1000311)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAbBWxwgTUZ2"
      },
      "source": [
        "**Building Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcL3D4NhS_qU",
        "outputId": "210c6096-328a-4528-86ba-2fead900875d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24296858"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from pyspark.sql.functions import broadcast, when\n",
        "\n",
        "train_data = user_artist_df.join(broadcast(artist_alias),\n",
        "                                              'artist', how='left')\n",
        "train_data = train_data.withColumn('artist',\n",
        "                                    when(col('alias').isNull(), col('artist')).\\\n",
        "                                    otherwise(col('alias')))\n",
        "\n",
        "train_data = train_data.withColumn('artist', col('artist').\\\n",
        "                                             cast(IntegerType())).\\\n",
        "                                             drop('alias')\n",
        "\n",
        "train_data.cache()\n",
        "\n",
        "train_data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCZAtWMCU5g6"
      },
      "source": [
        "We will use the Alternating Least Squares algorithm to compute latent factors from our dataset. This type of approach was popularized around the time of the Netflix Prize competition by papers like “Collaborative Filtering for Implicit Feedback Datasets” and “Large-Scale Parallel Collaborative Filtering for the Netflix Prize”. PySpark MLlib’s ALS implementation draws on ideas from both of these papers and is the only recommender algorithm currently implemented in Spark MLlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9ANiFboATiWU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "model = ALS(rank=10, seed=0, maxIter=5, regParam=0.1,\n",
        "            implicitPrefs=True, alpha=1.0, userCol='user',\n",
        "            itemCol='artist', ratingCol='count'). \\\n",
        "        fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0fNYICOT1vh",
        "outputId": "487650fd-37c8-4b87-fbdc-81a0d24f6373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------------------------------------------------------------------------------------------------+\n",
            "|id |features                                                                                                                   |\n",
            "+---+---------------------------------------------------------------------------------------------------------------------------+\n",
            "|90 |[0.16020626, 0.20717518, -0.1719469, 0.06038466, 0.06272771, 0.54658705, -0.4048189, 0.43657345, -0.10396772, -0.042728323]|\n",
            "+---+---------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.userFactors.show(1, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYUGVRaaXTyl"
      },
      "source": [
        "**Spot Checking Recommendations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ-ZaOENU7_N",
        "outputId": "bb354e2d-c1e4-4e05-e26e-36ea42cd9a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|     id|           name|\n",
            "+-------+---------------+\n",
            "|   1180|     David Gray|\n",
            "|    378|  Blackalicious|\n",
            "|    813|     Jurassic 5|\n",
            "|1255340|The Saw Doctors|\n",
            "|    942|         Xzibit|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_id = 2093760\n",
        "\n",
        "existing_artist_ids = train_data.filter(train_data.user == user_id).select(\"artist\").collect()\n",
        "\n",
        "existing_artist_ids = [i[0] for i in existing_artist_ids]\n",
        "\n",
        "artist_by_id.filter(col('id').isin(existing_artist_ids)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eelI53G5XZSG",
        "outputId": "1e29fa13-d3f2-4386-977f-909818b8577a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|   user|     recommendations|\n",
            "+-------+--------------------+\n",
            "|2093760|[[2814, 0.0294106...|\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_subset = train_data.select('user').where(col('user') == user_id).distinct()\n",
        "top_predictions = model.recommendForUserSubset(user_subset, 5)\n",
        "\n",
        "top_predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd6P12YeXeFH",
        "outputId": "318355c8-8441-425f-c554-58bca467eb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      user                                    recommendations\n",
            "0  2093760  [(2814, 0.029410675168037415), (1300642, 0.028...\n"
          ]
        }
      ],
      "source": [
        "top_predictions_pandas = top_predictions.toPandas()\n",
        "print(top_predictions_pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpVweKSuXkQP",
        "outputId": "efb2e6bc-f455-4885-824d-51351cdf8c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|     id|      name|\n",
            "+-------+----------+\n",
            "|   2814|   50 Cent|\n",
            "|   4605|Snoop Dogg|\n",
            "|1007614|     Jay-Z|\n",
            "|1001819|      2Pac|\n",
            "|1300642|  The Game|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "recommended_artist_ids = [i[0] for i in top_predictions_pandas.\\\n",
        "                                        recommendations[0]]\n",
        "\n",
        "artist_by_id.filter(col('id').isin(recommended_artist_ids)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bux-E_peXnbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad008558-2f7f-4b65-9256-7c87d7d32aa7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9039777085450373"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, lit, count, mean, coalesce\n",
        "from pyspark.sql import DataFrame\n",
        "from typing import List\n",
        "import random\n",
        "\n",
        "\n",
        "def area_under_curve(positive_data: DataFrame, b_all_artist_ids: List[int], predict_function) -> float:\n",
        "    positive_predictions = predict_function(positive_data.select(\"user\", \"artist\")).withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
        "\n",
        "    def negative_data_generation(user_artist_tuples):\n",
        "        user_negative_artists = []\n",
        "        for user, pos_artist_ids in user_artist_tuples:\n",
        "            pos_artist_id_set = set(pos_artist_ids)\n",
        "            negative_artists = set()\n",
        "            while len(negative_artists) < len(pos_artist_id_set):\n",
        "                artist_id = b_all_artist_ids[random.randint(0, len(b_all_artist_ids) - 1)]\n",
        "                if artist_id not in pos_artist_id_set:\n",
        "                    negative_artists.add(artist_id)\n",
        "            user_negative_artists.extend([(user, artist_id) for artist_id in negative_artists])\n",
        "        return user_negative_artists\n",
        "\n",
        "    user_artist_rdd = positive_data.select(\"user\", \"artist\").rdd.groupByKey().mapValues(list).collect()\n",
        "    negative_data = spark.createDataFrame(negative_data_generation(user_artist_rdd), schema=[\"user\", \"artist\"])\n",
        "\n",
        "    negative_predictions = predict_function(negative_data).withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
        "\n",
        "    joined_predictions = positive_predictions.join(negative_predictions, \"user\").select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
        "\n",
        "    all_counts = joined_predictions.groupBy(\"user\").agg(count(lit(1)).alias(\"total\")).select(\"user\", \"total\")\n",
        "    correct_counts = joined_predictions.filter(col(\"positivePrediction\") > col(\"negativePrediction\")).groupBy(\"user\").agg(count(\"user\").alias(\"correct\")).select(\"user\", \"correct\")\n",
        "\n",
        "    mean_auc = all_counts.join(correct_counts, [\"user\"], \"left_outer\").select(col(\"user\"), (coalesce(col(\"correct\"), lit(0)) / col(\"total\")).alias(\"auc\")).agg(mean(\"auc\")).collect()[0][0]\n",
        "\n",
        "    joined_predictions.unpersist()\n",
        "\n",
        "    return mean_auc\n",
        "\n",
        "\n",
        "\n",
        "all_data = user_artist_df.join(broadcast(artist_alias), 'artist', how='left') \\\n",
        "    .withColumn('artist', when(col('alias').isNull(), col('artist'))\\\n",
        "    .otherwise(col('alias'))) \\\n",
        "    .withColumn('artist', col('artist').cast(IntegerType())).drop('alias')\n",
        "\n",
        "train_data, cv_data = all_data.randomSplit([0.9, 0.1], seed=54321)\n",
        "train_data.cache()\n",
        "cv_data.cache()\n",
        "\n",
        "all_artist_ids = all_data.select(\"artist\").distinct()\n",
        "all_artist_ids = [i[0] for i in all_artist_ids.collect()]\n",
        "# b_all_artist_ids = broadcast(all_artist_ids)\n",
        "\n",
        "model = ALS(rank=10, seed=0, maxIter=5, regParam=0.1,\n",
        "            implicitPrefs=True, alpha=1.0, userCol='user',\n",
        "            itemCol='artist', ratingCol='count') \\\n",
        "        .fit(train_data)\n",
        "\n",
        "area_under_curve(cv_data, all_artist_ids, model.transform)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}